{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maharshi Trivedi\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Initialisation of libraries/directories\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as Img\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=cv2.imread('dataset1/images_prepped_train/0001TP_006690.png')[:,:,::-1]\n",
    "img=cv2.resize(img,dsize=(256,256),interpolation=cv2.INTER_NEAREST)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img=Img.imread('dataset1/annotations_prepped_train/0001TP_006690.png') \n",
    "img=cv2.resize(img,dsize=(256,256),interpolation=cv2.INTER_NEAREST)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_test_name = os.listdir('dataset1/images_prepped_test')\n",
    "image_train_name =os.listdir('dataset1/images_prepped_train')\n",
    "mask_train_name =os.listdir('dataset1/annotations_prepped_train') \n",
    "mask_test_name = os.listdir('dataset1/annotations_prepped_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_test = []\n",
    "image_train = []\n",
    "mask_test = []\n",
    "mask_train=[]\n",
    "for j,l in zip(image_train_name, mask_train_name):\n",
    "    img2 = cv2.imread(os.path.join('dataset1/images_prepped_train', j))[:,:,::-1]\n",
    "    img2 = cv2.resize(img2,dsize=(256,256),interpolation=cv2.INTER_NEAREST) #Inter_nearest is important to have 12 classes\n",
    "    img4 = Img.imread(os.path.join('dataset1/annotations_prepped_train',l))[:,::-1] #Img\n",
    "    img4 = cv2.resize(img4,dsize=(256,256),interpolation=cv2.INTER_NEAREST)\n",
    "    image_train.append(img2)\n",
    "    mask_train.append(img4)\n",
    "\n",
    "for i,k in zip(image_test_name,mask_test_name):\n",
    "    img1 =  cv2.imread(os.path.join('dataset1/images_prepped_test', i))[:,:,::-1]\n",
    "    img1 = cv2.resize(img1,dsize=(256,256),interpolation=cv2.INTER_NEAREST)\n",
    "    img3 = Img.imread(os.path.join('dataset1/annotations_prepped_test', k))[:,::-1]\n",
    "    a=img3\n",
    "    img3 = cv2.resize(img3,dsize=(256,256),interpolation=cv2.INTER_NEAREST)\n",
    "    image_test.append(img1)\n",
    "    mask_test.append(img3)\n",
    "    \n",
    "image_test = np.array(image_test)\n",
    "image_train = np.array(image_train)\n",
    "mask_test = np.resize(np.array(mask_test),(367,256,256,1))\n",
    "mask_train=np.resize(np.array(mask_train),(367,256,256,1))                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(mask_test[0][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(mask_train[225]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No. of unique labels/annotations\n",
    "labels=list(np.unique(mask_train[150]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.squeeze(mask_train[0])\n",
    "a[np.where(a==labels[1])]=1\n",
    "a[np.where(a!=1)]=0\n",
    "np.unique(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.squeeze(mask_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating mask from the label-annotations (Training Images)\n",
    "mask_train_bw=np.full((mask_train.shape[0],mask_train.shape[1],mask_train.shape[2],len(labels)),0)\n",
    "for image_no,m_train in enumerate(mask_train):\n",
    "    for index,label in enumerate(labels):\n",
    "        temp=np.full((mask_train.shape[1],mask_train.shape[2]),0)\n",
    "        temp[np.squeeze(m_train)==label]=1\n",
    "        mask_train_bw[image_no,:,:,index]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating mask from the label-annotations (Testing Images)\n",
    "mask_test_bw=np.full((mask_test.shape[0],mask_test.shape[1],mask_test.shape[2],len(labels)),0)\n",
    "for image_no,m_test in enumerate(mask_test):\n",
    "    for index,label in enumerate(labels):\n",
    "        temp=np.full((mask_test.shape[1],mask_test.shape[2]),0)\n",
    "        temp[np.squeeze(m_test)==label]=1\n",
    "        mask_test_bw[image_no,:,:,index]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask_test_bw[9][:,:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_train_bw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_test_bw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Convolution2D, ZeroPadding2D, MaxPooling2D, Deconvolution2D, Cropping2D\n",
    "from keras.layers import Input, Add, Dropout, Permute, add\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create to a series of CONV layers followed by Max pooling layer\n",
    "def Convblock(channel_dimension, block_no, no_of_convs) :\n",
    "    Layers = []\n",
    "    for i in range(no_of_convs) :\n",
    "        \n",
    "        Conv_name = \"conv\"+str(block_no)+\"_\"+str(i+1)\n",
    "        \n",
    "        # A constant kernel size of 3*3 is used for all convolutions\n",
    "        Layers.append(Convolution2D(channel_dimension,kernel_size = (3,3),padding = \"same\",activation = \"relu\",name = Conv_name))\n",
    "    \n",
    "    Max_pooling_name = \"pool\"+str(block_no)\n",
    "    \n",
    "    #Addding max pooling layer\n",
    "    Layers.append(MaxPooling2D(pool_size=(2, 2), strides=(2, 2),name = Max_pooling_name))\n",
    "    \n",
    "    return Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FCN_8_helper(image_size):\n",
    "    model = Sequential()\n",
    "    model.add(Permute((1,2,3),input_shape = (image_size,image_size,3)))\n",
    "    \n",
    "    for l in Convblock(64,1,2) :\n",
    "        model.add(l)\n",
    "    \n",
    "    for l in Convblock(128,2,2):\n",
    "        model.add(l)\n",
    "    \n",
    "    for l in Convblock(256,3,3):\n",
    "        model.add(l)\n",
    "    \n",
    "    for l in Convblock(512,4,3):\n",
    "        model.add(l)\n",
    "    \n",
    "    for l in Convblock(512,5,3):\n",
    "        model.add(l)\n",
    "        \n",
    "    model.add(Convolution2D(4096,kernel_size=(7,7),padding = \"same\",activation = \"relu\",name = \"fc6\"))\n",
    "      \n",
    "    #Replacing fully connnected layers of VGG Net using convolutions\n",
    "    model.add(Convolution2D(4096,kernel_size=(1,1),padding = \"same\",activation = \"relu\",name = \"fc7\"))\n",
    "    \n",
    "    # Gives the classifications scores for each of the 21 classes including background\n",
    "    model.add(Convolution2D(12,kernel_size=(1,1),padding=\"same\",activation=\"relu\",name = \"score_fr\"))\n",
    "    \n",
    "    Conv_size = model.layers[-1].output_shape[2] #16 if image size if 512\n",
    "    print(Conv_size)\n",
    "    \n",
    "    model.add(Deconvolution2D(12,kernel_size=(4,4),strides = (2,2),padding = \"valid\",activation=None,name = \"score2\"))\n",
    "    \n",
    "    # O = ((I-K+2*P)/Stride)+1 \n",
    "    # O = Output dimesnion after convolution\n",
    "    # I = Input dimnesion\n",
    "    # K = kernel Size\n",
    "    # P = Padding\n",
    "    \n",
    "    # I = (O-1)*Stride + K \n",
    "    Deconv_size = model.layers[-1].output_shape[2] #34 if image size is 512*512\n",
    "    \n",
    "    #print(Deconv_size)\n",
    "    # 2 if image size is 512*512\n",
    "    Extra = (Deconv_size - 2*Conv_size)\n",
    "    \n",
    "    #print(Extra)\n",
    "    \n",
    "    #Cropping to get correct size\n",
    "    model.add(Cropping2D(cropping=((0,Extra),(0,Extra))))\n",
    "    \n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FCN_8(image_size):\n",
    "    fcn_8 = FCN_8_helper(image_size)\n",
    "    #Calculating conv size after the sequential block\n",
    "    #32 if image size is 512*512\n",
    "    Conv_size = fcn_8.layers[-1].output_shape[2] \n",
    "    \n",
    "    #Conv to be applied on Pool4\n",
    "    skip_con1 = Convolution2D(12,kernel_size=(1,1),padding = \"same\",activation=None, name = \"score_pool4\")\n",
    "    \n",
    "    #Addig skip connection which takes adds the output of Max pooling layer 4 to current layer\n",
    "    Summed = add(inputs = [skip_con1(fcn_8.layers[14].output),fcn_8.layers[-1].output])\n",
    "    \n",
    "    #Upsampling output of first skip connection\n",
    "    x = Deconvolution2D(12,kernel_size=(4,4),strides = (2,2),padding = \"valid\",activation=None,name = \"score4\")(Summed)\n",
    "    x = Cropping2D(cropping=((0,2),(0,2)))(x)\n",
    "    \n",
    "    \n",
    "    #Conv to be applied to pool3\n",
    "    skip_con2 = Convolution2D(12,kernel_size=(1,1),padding = \"same\",activation=None, name = \"score_pool3\")\n",
    "    \n",
    "    #Adding skip connection which takes output og Max pooling layer 3 to current layer\n",
    "    Summed = add(inputs = [skip_con2(fcn_8.layers[10].output),x])\n",
    "    \n",
    "    #Final Up convolution which restores the original image size\n",
    "    Up = Deconvolution2D(12,kernel_size=(16,16),strides = (8,8),\n",
    "                         padding = \"valid\",activation = None,name = \"upsample\")(Summed)\n",
    "    \n",
    "    #Cropping the extra part obtained due to transpose convolution\n",
    "    final = Cropping2D(cropping = ((0,8),(0,8)))(Up)\n",
    "    \n",
    "    \n",
    "    return Model(fcn_8.input, final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCN_8(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.metrics import categorical_crossentropy\n",
    "model.compile(optimizer=Adam(lr=.0001), loss= 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=image_train,y=mask_train_bw, batch_size=50, epochs=2, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
